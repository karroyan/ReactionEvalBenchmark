#!/usr/bin/env python3
import random
from typing import List
from music_appraisal_benchmark import run_comprehensive_benchmark


def demo_qa_model(audio_path: str, question: str, options: List[str]) -> str:
    """
    A sample of answer to qa question
    """
    # For demo purposes, use some simple heuristics
    question_lower = question.lower()
    
    answers = [
        "After careful listening, I believe the answer is A.",
        "My analysis suggests the correct choice is B.",
        "Based on the musical features, I think C is the right answer.",
        "The audio evidence points to option D."
    ]
    return random.choice(answers)


def demo_appraisal_model(audio_path: str) -> str:
    """
    A sample of answer to reaction question
    """
    # Generate different types of appraisals to test all evaluation components
    # appraisals = [
    #     # High quality appraisal
    #     """è¿™é¦–ã€Šå€’å¸¦ã€‹æ˜¯è”¡ä¾æ—åœ¨2004å¹´å‘å¸ƒçš„ç»å…¸ä½œå“ï¼Œæ”¶å½•åœ¨å¥¹çš„ä¸“è¾‘ã€ŠåŸå ¡ã€‹ä¸­ã€‚ä½†è¿™é¦–æ­Œå¯¹æˆ‘æ¥è¯´ä¸ä»…ä»…æ˜¯ä¸€é¦–æµè¡Œæ­Œæ›²ï¼Œå®ƒåƒæ˜¯ä¸€å°æƒ…æ„Ÿçš„æ—¶å…‰æœºã€‚
    #         å¬åˆ°è¿™é¦–æ­Œçš„ç¬¬ä¸€ç§’ï¼Œæˆ‘å°±ä»¿ä½›è¢«å¸¦å›åˆ°äº†é‚£ä¸ªé’æ¶©çš„å¹´ä»£ã€‚è”¡ä¾æ—çš„å£°éŸ³åœ¨è¿™é‡Œæœ‰ç§ç‰¹æ®Šçš„æ¸©æŸ”ï¼Œå°±åƒä¸ç»¸åˆ’è¿‡æ°´é¢ä¸€æ ·é¡ºæ»‘ï¼Œåˆå¸¦ç€ä¸€ç§æ·¡æ·¡çš„å¿§éƒã€‚ç‰¹åˆ«æ˜¯å‰¯æ­Œéƒ¨åˆ†"å¦‚æœæ—¶é—´èƒ½å¤Ÿå€’å¸¦"ï¼Œå¥¹çš„éŸ³è‰²ä»¿ä½›åœ¨è€³è¾¹è½»æŠšï¼Œè®©äººä¸ç¦æƒ³èµ·è‡ªå·±çš„åˆæ‹å›å¿†ã€‚
    #         ä»åˆ¶ä½œè§’åº¦æ¥è¯´ï¼Œè¿™é¦–æ­Œçš„ç¼–æ›²å±‚æ¬¡éå¸¸ä¸°å¯Œã€‚é¼“ç‚¹çš„å¤„ç†ç‰¹åˆ«å·§å¦™ï¼Œæ¯ä¸€ä¸‹å‡»æ‰“éƒ½åƒå¿ƒè·³ä¸€æ ·è§„å¾‹è€Œæ·±æ²‰ï¼Œä½†åˆä¸ä¼šæŠ¢å¤ºäººå£°çš„é£å¤´ã€‚å¼¦ä¹çš„è¿ç”¨æ›´æ˜¯ç”»é¾™ç‚¹ç›ï¼Œåœ¨å…³é”®çš„æƒ…æ„Ÿè½¬æŠ˜å¤„ï¼Œé‚£äº›å¼¦ä¹å°±åƒçœ¼æ³ªä¸€æ ·ç¼“ç¼“æµæ·Œä¸‹æ¥ã€‚
    #         æˆ‘è§‰å¾—è¿™é¦–æ­Œå’Œç‹è²çš„ã€Šçº¢è±†ã€‹æœ‰å¼‚æ›²åŒå·¥ä¹‹å¦™ï¼Œéƒ½æ˜¯ç”¨ç®€å•çš„æ—‹å¾‹æ‰¿è½½å¤æ‚çš„æƒ…æ„Ÿã€‚ä½†ã€Šå€’å¸¦ã€‹æ›´å¤šäº†ä¸€ç§é’æ˜¥çš„èºåŠ¨ï¼Œé‚£ç§æƒ³è¦é‡æ–°æ¥è¿‡ä½†åˆæ— èƒ½ä¸ºåŠ›çš„çŸ›ç›¾å¿ƒæƒ…ï¼Œè¢«è¡¨è¾¾å¾—æ·‹æ¼“å°½è‡´ã€‚
    #         ä»æ–‡åŒ–è§’åº¦æ¥çœ‹ï¼Œè¿™é¦–æ­Œå‡ºç°åœ¨2004å¹´ï¼Œæ­£å¥½æ˜¯åè¯­æµè¡ŒéŸ³ä¹é»„é‡‘æ—¶ä»£çš„å°¾å£°ã€‚å®ƒæ—¢ä¿ç•™äº†90å¹´ä»£æƒ…æ­Œçš„æ·±æ²‰ï¼Œåˆèå…¥äº†æ–°ä¸–çºªçš„åˆ¶ä½œç†å¿µï¼Œå¯ä»¥è¯´æ˜¯æ‰¿ä¸Šå¯ä¸‹çš„ç»å…¸ä¹‹ä½œã€‚
    #         æ¯æ¬¡å¬åˆ°è¿™é¦–æ­Œï¼Œæˆ‘éƒ½ä¼šæƒ³èµ·é‚£ä¸ªä¸‹ç€å°é›¨çš„å‚æ™šï¼Œæˆ‘å’Œæœ‹å‹åœ¨KTVé‡Œä¸€ééåœ°å¾ªç¯æ’­æ”¾ï¼Œé‚£ç§é’æ˜¥çš„ç¾å¥½å’Œé—æ†¾äº¤ç»‡åœ¨ä¸€èµ·çš„æ„Ÿè§‰ï¼Œå°±åƒæ­Œè¯é‡Œè¯´çš„é‚£æ ·ï¼ŒçœŸå¸Œæœ›æ—¶é—´èƒ½å¤Ÿå€’å¸¦ã€‚""",
        
    #     # Medium novelty
    #     """è”¡ä¾æ—çš„ã€Šå€’å¸¦ã€‹ç¡®å®æ˜¯å¥¹è½¬å‹æœŸçš„ä»£è¡¨ä½œä¹‹ä¸€ã€‚è¿™é¦–æ­Œåœ¨2004å¹´æ¨å‡ºæ—¶ï¼Œæˆ‘è¿˜åœ¨ä¸Šé«˜ä¸­ï¼Œå½“æ—¶å°±è§‰å¾—è¿™é¦–æ­Œæœ‰ç§ç‰¹åˆ«çš„é­…åŠ›ã€‚
    #         éŸ³ä¹åˆ¶ä½œä¸Šï¼Œæ•´é¦–æ­Œçš„ç¼–æ›²å¾ˆç²¾è‡´ï¼Œç‰¹åˆ«æ˜¯é¼“ç‚¹çš„è®¾è®¡å¾ˆæœ‰å±‚æ¬¡æ„Ÿã€‚è”¡ä¾æ—çš„æ¼”å”±æŠ€å·§åœ¨è¿™é¦–æ­Œä¸­å±•ç°å¾—å¾ˆå¥½ï¼Œå¥¹çš„éŸ³è‰²æ—¢æ¸…ç”œåˆæœ‰ä¸€å®šçš„æˆç†Ÿåº¦ã€‚
    #         æ­Œæ›²çš„ä¸»é¢˜å…³äºå›å¿†å’Œæ—¶å…‰å€’æµï¼Œå¾ˆå®¹æ˜“å¼•èµ·å¬ä¼—çš„å…±é¸£ã€‚æˆ‘è§‰å¾—è¿™é¦–æ­Œæ¯”å¥¹ä¹‹å‰çš„ä¸€äº›ä½œå“æ›´æœ‰æ·±åº¦ï¼Œåœ¨æƒ…æ„Ÿè¡¨è¾¾ä¸Šæœ‰äº†æ˜æ˜¾çš„æå‡ã€‚æ•´ä½“æ¥è¯´æ˜¯ä¸€é¦–å¾ˆæˆåŠŸçš„æµè¡Œæ­Œæ›²ã€‚""",
        
    #     # Low novelty
    #     """è¿™é¦–ã€Šå€’å¸¦ã€‹æ˜¯è”¡ä¾æ—æ¼”å”±çš„æ­Œæ›²ï¼ŒéŸ³ä¹é£æ ¼å±äºæµè¡Œç±»å‹ã€‚æ­Œæ›²çš„ç¼–æ›²æ¯”è¾ƒä¸é”™ï¼Œè”¡ä¾æ—çš„æ¼”å”±ä¹Ÿå¾ˆä¸“ä¸šã€‚
    #         è¿™é¦–æ­Œè¡¨è¾¾äº†å¯¹å›å¿†çš„æ€€å¿µï¼Œæ˜¯ä¸€é¦–æ¯”è¾ƒç»å…¸çš„ä½œå“ã€‚æ•´ä½“å¬èµ·æ¥å¾ˆèˆ’æœï¼Œæ˜¯ä¸€é¦–å€¼å¾—æ¨èçš„å¥½æ­Œã€‚"""
    # ]
    appraisals = ["è¿™é¦–æ­Œæ˜¯ä¸€é¦–å™¨ä¹æ›²ï¼Œä¸»è¦ç‰¹è‰²æ˜¯å‰ä»–ã€‚å‰ä»–åœ¨æ›²ç›®ä¸­æ‰®æ¼”äº†ä¸»å¯¼çš„è§’è‰²ï¼Œç”¨ä¸åŒæ•ˆæœå™¨å¤„ç†è¿‡çš„å£°éŸ³åˆ›é€ å‡ºç‹¬ç‰¹çš„æ°›å›´ã€‚ä»æŸ”å’Œè€Œæ¢¦å¹»çš„å¼€å¤´é€æ¸è¿‡æ¸¡åˆ°æ›´åŠ å¼ºçƒˆå’Œæœ‰åŠ›çš„éƒ¨åˆ†ï¼Œå±•ç¤ºäº†å‰ä»–çš„å˜åŒ–å’Œé­…åŠ›ã€‚æ•´ä½“è€Œè¨€ï¼Œè¿™é¦–æ­Œç»™äººçš„æ„Ÿè§‰éå¸¸å®å¤§å’Œæ¿€åŠ±äººå¿ƒï¼Œé€‚åˆåšæŸç§ç³»åˆ—èŠ‚ç›®çš„èƒŒæ™¯éŸ³ä¹ã€‚"]

    # Randomly select an appraisal to test different novelty levels
    return random.choice(appraisals)


def main():
    print("Music Appraisal Comprehensive Benchmark Demo")
    print("=" * 60)
    print("This demo showcases all evaluation components:")
    print("1. Option-based QA evaluation")
    print("2. LLM-based completeness scoring")
    print("3. Precision evaluation against ground truth")
    print("4. Novelty/detail assessment")
    print("5. Overall performance assessment")
    print("=" * 60)
    
    random.seed(42)
    
    try:
        # Run the comprehensive benchmark with all components enabled
        result = run_comprehensive_benchmark(
            qa_model_function=demo_qa_model,
            appraisal_model_function=demo_appraisal_model,
            qa_data_path="/fs-computility/niuyazhe/lixueyan/acapella/eval_benchmark/data/option_qa.jsonl",
            song_details_path="data/song_details.jsonl",
            output_path="/fs-computility/niuyazhe/lixueyan/acapella/eval_benchmark/comprehensive_demo_results.json",
            enable_precision_eval=True,  # Enable precision evaluation
            enable_novelty_eval=True     # Enable novelty evaluation
        )
        
        print("\nDemo completed successfully!")
        print("Check 'comprehensive_demo_results.json' for detailed results.")
        
        # Show detailed analysis
        print(f"\n" + "="*60)
        print("ğŸµ DEMO ANALYSIS SUMMARY")
        print("="*60)
        
        print(f"ğŸ“Š Question Answering Performance:")
        print(f"   - Accuracy: {result.qa_accuracy:.1%}")
        print(f"   - Questions answered: {result.qa_correct_answers}/{result.qa_total_questions}")
        
        print(f"\nğŸ“ Completeness Evaluation:")
        print(f"   - Score: {result.completeness_score:.1f}/16.0")
        print(f"   - Percentage: {result.completeness_score/16*100:.0f}%")
        
        if result.precision_score is not None:
            print(f"\nğŸ¯ Precision Evaluation:")
            print(f"   - Factual accuracy: {result.precision_score:.1%}")
            if result.precision_details:
                details = result.precision_details
                print(f"   - Total factual claims: {details.get('total_claims', 0)}")
                print(f"   - Correct claims: {details.get('total_correct_claims', 0)}")
                print(f"   - Incorrect claims: {details.get('total_incorrect_claims', 0)}")
        else:
            print(f"\nğŸ¯ Precision Evaluation: Skipped (no ground truth)")
        
        if result.novelty_score is not None:
            print(f"\nâœ¨ Novelty Evaluation:")
            print(f"   - Overall novelty: {result.novelty_score:.1%}")
            if result.novelty_details:
                details = result.novelty_details
                print(f"   - Music relevance: {details.get('avg_music_relevance_score', 0):.1%}")
                print(f"   - Depth score: {details.get('avg_depth_score', 0):.1%}")
                print(f"   - Personal insight: {details.get('avg_personal_insight_score', 0):.1%}")
                print(f"   - Novel statements: {details.get('total_novel_statements', 0)}")
                
                # Show insight types found
                insight_counts = details.get('insight_type_counts', {})
                files_count = details.get('files_evaluated', 1)
                print(f"\n   ğŸ’¡ Insight Types Found:")
                print(f"     ğŸ­ Personal reactions: {insight_counts.get('has_personal_reactions', 0)}/{files_count}")
                print(f"     ğŸ”§ Technical analysis: {insight_counts.get('has_technical_analysis', 0)}/{files_count}")
                print(f"     ğŸ¨ Creative interpretations: {insight_counts.get('has_creative_interpretations', 0)}/{files_count}")
                print(f"     ğŸŒ Cultural context: {insight_counts.get('has_cultural_context', 0)}/{files_count}")
                print(f"     ğŸ“Š Comparative analysis: {insight_counts.get('has_comparative_analysis', 0)}/{files_count}")
        else:
            print(f"\nâœ¨ Novelty Evaluation: Skipped (no ground truth)")
        
        print(f"\nğŸ† Overall Performance: {result.overall_score:.1%}")
        
        print(f"\n" + "="*60)
        print("ğŸ” EVALUATION INSIGHTS:")
        print("="*60)
        print("â€¢ QA component tests specific music knowledge and comprehension")
        print("â€¢ Completeness scoring evaluates comprehensive coverage of required elements")
        print("â€¢ Precision evaluation checks factual accuracy against verified information")
        print("â€¢ Novelty assessment rewards personal insights and creative interpretations")
        print("â€¢ Combined metrics provide holistic assessment of music appraisal quality")
        
        # Provide performance insights
        if result.precision_details:
            avg_precision = result.precision_details.get('average_score', 0)
            if avg_precision > 0.8:
                print("âœ… High precision - model makes accurate factual claims")
            elif avg_precision > 0.5:
                print("âš ï¸  Medium precision - some factual errors detected")
            else:
                print("âŒ Low precision - significant factual inaccuracies")
        
        if result.novelty_details:
            avg_novelty = result.novelty_details.get('avg_novelty_score', 0)
            if avg_novelty > 0.7:
                print("ğŸŒŸ High novelty - rich personal insights and creative interpretations")
            elif avg_novelty > 0.4:
                print("ğŸ’« Medium novelty - some personal elements and depth")
            else:
                print("ğŸ“ Low novelty - mostly basic factual content")
        
        print(f"\nğŸ¯ The benchmark successfully evaluates music appraisal models across")
        print(f"   knowledge accuracy, content completeness, factual precision, and creative depth!")
        
    except Exception as e:
        print(f"Error running benchmark: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 