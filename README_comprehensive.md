# Comprehensive Music Appraisal Benchmark

A comprehensive evaluation framework for audio models that generate music appraisals and answer music-related questions. This benchmark evaluates models across multiple dimensions to provide a holistic assessment of their music understanding and appraisal capabilities.

## ğŸ¯ Evaluation Components

### 1. Option-based Question Answering (30% weight)
- **Purpose**: Tests specific music knowledge and comprehension
- **Method**: Multiple-choice questions about song structure, style, and content
- **Scoring**: Accuracy percentage based on correct answers
- **Features**: Flexible answer extraction from verbose model responses

### 2. LLM-based Completeness Scoring (40% weight) 
- **Purpose**: Evaluates whether appraisals contain all required elements
- **Method**: Uses LLM to score content across 4 dimensions (16-point scale):
  - éŸ³ä¹ç†è§£ (Music Understanding): 7 points
  - èƒŒæ™¯åŠæƒ…æ™¯ç†è§£ (Background Understanding): 4 points  
  - è¯­è¨€è¡¨è¾¾ (Language Expression): 2 points
  - äººè®¾ç±»å‹ (Persona Consistency): 3 points
- **Scoring**: Total score out of 16 points with detailed breakdown

### 3. Precision Evaluation (20% weight)
- **Purpose**: Checks factual accuracy of claims made in appraisals
- **Method**: Uses LLM to compare model outputs with ground truth song details
- **Focus**: Only evaluates accuracy of mentioned facts (not completeness)
- **Features**: 
  - Identifies all factual claims in appraisal text
  - Compares each claim against verified ground truth
  - Calculates precision score: correct claims / total claims
  - Provides detailed breakdown of correct vs incorrect facts

### 4. Novelty Assessment (10% weight) âœ¨ **NEW**
- **Purpose**: Rewards detailed, personal, and creative insights beyond basic facts
- **Method**: Uses LLM to identify and evaluate novel content in 5 categories
- **Focus**: Music-related novelty, depth of insight, and personal perspectives
- **Categories**:
  - **Personal Reactions**: Emotional responses, memories, personal connections
  - **Technical Analysis**: Detailed production insights, instrumentation, arrangement
  - **Creative Interpretations**: Metaphors, artistic descriptions, sensory language
  - **Cultural Context**: Historical significance, social impact, era analysis
  - **Comparative Analysis**: Connections to other artists, songs, or genres

## ğŸš€ Quick Start

### Basic Usage

```python
from music_appraisal_benchmark import run_comprehensive_benchmark

def my_qa_model(audio_path: str, question: str, options: List[str]) -> str:
    # Your QA model implementation
    return "I think the answer is A"

def my_appraisal_model(audio_path: str) -> str:
    # Your appraisal generation model
    return "This song by è”¡ä¾æ— is a beautiful ballad..."

# Run comprehensive evaluation
result = run_comprehensive_benchmark(
    qa_model_function=my_qa_model,
    appraisal_model_function=my_appraisal_model,
    qa_data_path="data/option_qa.jsonl",
    song_details_path="data/song_details.jsonl",  # Required for precision & novelty eval
    enable_precision_eval=True,
    enable_novelty_eval=True
)

print(f"Overall Score: {result.overall_score:.2%}")
print(f"QA Accuracy: {result.qa_accuracy:.2%}")
print(f"Completeness: {result.completeness_score:.1f}/16")
print(f"Precision: {result.precision_score:.2%}")
print(f"Novelty: {result.novelty_score:.2%}")
```

### Advanced Usage

```python
from music_appraisal_benchmark import MusicAppraisalBenchmark

# Initialize benchmark with custom settings
benchmark = MusicAppraisalBenchmark(
    qa_data_path="data/option_qa.jsonl",
    song_details_path="data/song_details.jsonl",
    llm_api_key="your-api-key",
    llm_base_url="your-llm-endpoint"
)

# Run evaluation with custom options
result = benchmark.evaluate_model(
    qa_model_function=my_qa_model,
    appraisal_model_function=my_appraisal_model,
    enable_precision_eval=True,
    enable_novelty_eval=True,
    include_qa_confidence=True,
    qa_confidence_function=my_confidence_model
)

# Get detailed report
benchmark.print_detailed_report(result)
benchmark.save_results(result, "results.json")
```

## ğŸ“Š Data Formats

### QA Data Format (`option_qa.jsonl`)
```json
{
  "audio_path": "/path/to/audio.mp3",
  "question": "è¿™é¦–æ­Œçš„æ®µè½ç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿ",
  "options": ["verse", "chorus", "bridge", "outro"],
  "correct_answer": "A"
}
```

### Song Details Format (`song_details.jsonl`) - For Precision & Novelty Evaluation
```json
{
  "audio_path": "/path/to/audio.mp3",
  "artist": "è”¡ä¾æ—",
  "title": "å€’å¸¦", 
  "release_year": 2004,
  "album": "åŸå ¡",
  "genre": "æµè¡Œ",
  "theme": "å›å¿†ä¸æ—¶å…‰å€’æµ",
  "background": "è¡¨è¾¾å¯¹è¿‡å¾€æ‹æƒ…çš„æ€€å¿µ",
  "music_style": "èåˆæµè¡Œå’Œç”µå­å…ƒç´ ",
  "vocal_characteristics": "æ¸…ç”œä¸­å¸¦æœ‰æˆç†ŸéŸµå‘³"
}
```

## ğŸ¯ Precision Evaluation Details

The precision evaluation component focuses on **factual accuracy** rather than completeness. It:

1. **Extracts Factual Claims**: Identifies specific factual statements in the appraisal
2. **Compares with Ground Truth**: Checks each claim against verified song information  
3. **Ignores Subjective Content**: Skips personal opinions like "sounds good" or "very moving"
4. **Calculates Precision**: Reports the percentage of factual claims that are correct

### Example Precision Analysis

**Model Output:**
> "è¿™é¦–ã€Šå€’å¸¦ã€‹æ˜¯è”¡ä¾æ—åœ¨2004å¹´å‘å¸ƒçš„ç»å…¸ä½œå“ï¼Œæ”¶å½•åœ¨ä¸“è¾‘ã€ŠåŸå ¡ã€‹ä¸­ã€‚æ­Œæ›²é‡‡ç”¨äº†R&Bé£æ ¼ï¼Œè¡¨è¾¾äº†å¯¹å›å¿†çš„æ€€å¿µã€‚"

**Ground Truth:**
- Release year: 2004 âœ“
- Album: åŸå ¡ âœ“  
- Genre: æµè¡Œ (not R&B) âœ—
- Theme: å›å¿† âœ“

**Precision Score:** 3/4 = 75%

## âœ¨ Novelty Evaluation Details

The novelty evaluation component focuses on **creative depth and personal insights** beyond basic facts. It:

1. **Identifies Novel Content**: Finds content that goes beyond ground truth information
2. **Categorizes by Type**: Classifies novel content into 5 insight categories
3. **Evaluates Music Relevance**: Ensures novel content is actually about music
4. **Scores Depth**: Assesses the insight level from surface to deep analysis

### Novel Content Categories

#### 1. Personal Reactions ğŸ­
- Emotional responses: "è¿™é¦–æ­Œè®©æˆ‘æƒ³èµ·..."
- Memory connections: "å¬åˆ°æ—¶æˆ‘ä»¿ä½›å›åˆ°..."
- Personal interpretations: "å¯¹æˆ‘æ¥è¯´è¿™é¦–æ­Œä»£è¡¨..."

#### 2. Technical Analysis ğŸ”§
- Production details: "é¼“ç‚¹çš„å¤„ç†ç‰¹åˆ«å·§å¦™"
- Instrumentation insights: "å¼¦ä¹çš„è¿ç”¨ç”»é¾™ç‚¹ç›"
- Arrangement specifics: "å‰¯æ­Œéƒ¨åˆ†çš„å’Œå£°å±‚æ¬¡"

#### 3. Creative Interpretations ğŸ¨
- Metaphors: "å£°éŸ³å¦‚ä¸ç»¸èˆ¬é¡ºæ»‘"
- Artistic descriptions: "é¼“ç‚¹åƒå¿ƒè·³ä¸€æ ·è§„å¾‹"
- Sensory language: "éŸ³è‰²ä»¿ä½›åœ¨è€³è¾¹è½»æŠš"

#### 4. Cultural Context ğŸŒ
- Historical significance: "åè¯­æµè¡ŒéŸ³ä¹é»„é‡‘æ—¶ä»£çš„å°¾å£°"
- Social impact: "ä»£è¡¨äº†é‚£ä¸ªæ—¶ä»£çš„æƒ…æ„Ÿè¡¨è¾¾"
- Era analysis: "æ‰¿ä¸Šå¯ä¸‹çš„ç»å…¸ä¹‹ä½œ"

#### 5. Comparative Analysis ğŸ“Š
- Artist comparisons: "å’Œç‹è²çš„ã€Šçº¢è±†ã€‹æœ‰å¼‚æ›²åŒå·¥ä¹‹å¦™"
- Genre connections: "è®©æˆ‘æƒ³èµ·äº†æ—¥æœ¬çš„City Pop"
- Style evolution: "æ¯”å¥¹ä¹‹å‰çš„ä½œå“æ›´æœ‰æ·±åº¦"

### Example Novelty Analysis

**Model Output:**
> "å¬åˆ°è¿™é¦–æ­Œçš„ç¬¬ä¸€ç§’ï¼Œæˆ‘å°±ä»¿ä½›è¢«å¸¦å›åˆ°äº†é‚£ä¸ªé’æ¶©çš„å¹´ä»£ã€‚è”¡ä¾æ—çš„å£°éŸ³å°±åƒä¸ç»¸åˆ’è¿‡æ°´é¢ä¸€æ ·é¡ºæ»‘ï¼Œé¼“ç‚¹çš„å¤„ç†ç‰¹åˆ«å·§å¦™ï¼Œæ¯ä¸€ä¸‹å‡»æ‰“éƒ½åƒå¿ƒè·³ä¸€æ ·è§„å¾‹ã€‚æˆ‘è§‰å¾—è¿™é¦–æ­Œå’Œç‹è²çš„ã€Šçº¢è±†ã€‹æœ‰å¼‚æ›²åŒå·¥ä¹‹å¦™ã€‚"

**Novelty Analysis:**
- Personal Reactions: "ä»¿ä½›è¢«å¸¦å›åˆ°é‚£ä¸ªé’æ¶©çš„å¹´ä»£" âœ“
- Creative Interpretations: "åƒä¸ç»¸åˆ’è¿‡æ°´é¢", "åƒå¿ƒè·³ä¸€æ ·è§„å¾‹" âœ“âœ“  
- Technical Analysis: "é¼“ç‚¹çš„å¤„ç†ç‰¹åˆ«å·§å¦™" âœ“
- Comparative Analysis: "å’Œç‹è²çš„ã€Šçº¢è±†ã€‹æœ‰å¼‚æ›²åŒå·¥ä¹‹å¦™" âœ“

**Novelty Score:** High (5 novel statements, all music-related)

### Scoring Criteria (10-point scale)
- **Novelty Score** (0-4 points): Amount of new information beyond basic facts
- **Music Relevance** (0-3 points): How music-related the novel content is
- **Insight Depth** (0-3 points): Depth and uniqueness of analysis

## ğŸ“ˆ Evaluation Results

### Result Structure
```python
@dataclass
class ComprehensiveEvaluationResult:
    # QA metrics
    qa_accuracy: float
    qa_total_questions: int
    qa_correct_answers: int
    
    # Completeness metrics  
    completeness_score: float
    completeness_max_score: float
    completeness_assessment: str
    
    # Precision metrics
    precision_score: float
    precision_details: Dict[str, Any]
    
    # Novelty metrics âœ¨ NEW
    novelty_score: float
    novelty_details: Dict[str, Any]
    
    # Overall assessment
    overall_score: float
    summary: str
```

### Novelty Details Structure
```python
{
    "avg_music_relevance_score": 0.85,
    "avg_depth_score": 0.72,
    "avg_personal_insight_score": 0.68,
    "total_novel_statements": 25,
    "total_music_related_statements": 23,
    "music_relevance_ratio": 0.92,
    "novel_content_categories": {
        "personal_reactions": [...],
        "technical_analysis": [...],
        "creative_interpretations": [...],
        "cultural_context": [...],
        "comparative_analysis": [...]
    },
    "insight_type_counts": {
        "has_personal_reactions": 2,
        "has_technical_analysis": 1,
        "has_creative_interpretations": 2,
        "has_cultural_context": 1,
        "has_comparative_analysis": 1
    }
}
```

## âš™ï¸ Configuration

### Scoring Weights
- Option-based QA: 30%
- Completeness: 40%  
- Precision: 20%
- Novelty: 10%

### LLM Settings
- Default model: `deepseek-v3`
- Temperature: 0.6 (completeness), 0.3 (precision), 0.4 (novelty)
- Max tokens: 4096

## ğŸ”§ Requirements

```bash
pip install openai pandas numpy
```

## ğŸ“ Example Output

```
COMPREHENSIVE MUSIC APPRAISAL EVALUATION REPORT
================================================================================

1. OPTION-BASED QUESTION ANSWERING
   Accuracy: 0.7500 (75.00%)
   Correct: 3/4

2. COMPLETENESS EVALUATION  
   Score: 13.50/16.00
   Percentage: 84.4%
   Assessment: å†…å®¹è¾ƒä¸ºå®Œæ•´ï¼Œæ¶µç›–äº†ä¸»è¦è¯„ä»·ç»´åº¦

3. PRECISION EVALUATION
   Overall Accuracy: 0.8000 (80.00%)
   Total Claims: 15
   Correct Claims: 12
   Incorrect Claims: 3
   Files Evaluated: 2

4. NOVELTY EVALUATION
   Overall Novelty: 0.7200 (72.00%)
   Music Relevance: 0.85
   Depth Score: 0.72
   Personal Insight: 0.68
   Novel Statements: 25
   Music-Related Ratio: 92.0%

   Insight Types Found:
     Personal Reactions: 2/2
     Technical Analysis: 1/2
     Creative Interpretations: 2/2
     Cultural Context: 1/2
     Comparative Analysis: 1/2

5. OVERALL ASSESSMENT
   Overall Score: 0.7775 (77.75%)

æ¨¡å‹ç»¼åˆè¯„ä¼°ç»“æœï¼š
- é€‰æ‹©é¢˜å‡†ç¡®ç‡: 75.00% (3/4)
- å®Œæ•´æ€§è¯„åˆ†: 13.5/16.0 (84.4%)
- äº‹å®å‡†ç¡®ç‡: 80.00%
- æ–°é¢–æ€§å¾—åˆ†: 72.00%
- ç»¼åˆå¾—åˆ†: 77.75%
================================================================================
```

## ğŸš€ Running the Demo

```bash
cd eval_benchmark
python demo_comprehensive_benchmark.py
```

The demo showcases all evaluation components and provides insights into model performance across different dimensions, including novelty assessment.

## ğŸ” Key Features

- **Flexible Answer Extraction**: Handles verbose model responses with embedded answers
- **Multi-dimensional Evaluation**: Comprehensive assessment across knowledge, completeness, accuracy, and creativity
- **Detailed Reporting**: Rich output with component-wise analysis and insights
- **Novelty Recognition**: Identifies and categorizes creative insights and personal perspectives
- **Music-Relevance Filtering**: Ensures novel content is actually about music aspects
- **Extensible Architecture**: Easy to add new evaluation components
- **Robust Error Handling**: Graceful handling of API failures and parsing errors
- **Ground Truth Integration**: Uses verified song information for precision and novelty evaluation

## ğŸ“‹ Future Enhancements

1. **Confidence Calibration**: Evaluate model confidence accuracy across all components
2. **Multi-language Support**: Extend evaluation to other languages
3. **Audio Feature Integration**: Incorporate actual audio analysis results
4. **Comparative Analysis**: Compare multiple models side-by-side
5. **Dynamic Weighting**: Adjust component weights based on use case requirements
6. **Semantic Similarity**: Use embeddings for more nuanced novelty detection 